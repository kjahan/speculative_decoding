{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative Sampling\n",
        "\n",
        "Here our goal is to speed up generative model inference time. This has many use cases for edit suggestion for writing or coding.\n",
        "\n",
        "We will use a draft/basic decoder like GPT-2 and a bigger size model as the core model. We will prompt the draft model and generate k speculative tokens along with their probabilities.\n",
        "\n",
        "Next we feed those k tokens along with the original prompt to the main model to get their liklihhods at once from the attention mask layer. Then we use the probabilities for speculative tokens from the draft model and main model to accept or reject speculated tokens.\n",
        "\n",
        "See this video for more explanations:\n",
        "\n",
        "https://www.youtube.com/watch?v=S-8yr_RibJ4\n",
        "\n",
        "The key insight is that there are many simple tokens like \"of\" that even smaller model can easily predict them so we can use the smaller model to generate them faster and then use the bigger size model for facts and harder tokens!\n",
        "\n",
        "https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/\n",
        "\n",
        "https://www.youtube.com/watch?v=9wNAgpX6z_4\n",
        "\n",
        "https://docs.google.com/presentation/d/1p1xE-EbSAnXpTSiSI0gmy_wdwxN5XaULO3AnCWWoRe4/edit#slide=id.p"
      ],
      "metadata": {
        "id": "CxOfeD1PkeYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load draft model\n",
        "\n",
        "`GPT-2`"
      ],
      "metadata": {
        "id": "kJBAkfFAa4hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "04yqPITqmCfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained tokenizer and model for text generation\n",
        "tokenizer_draft = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model_draft = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Move model and input to the same device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_draft.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDYLeeven4Zv",
        "outputId": "7fd27d66-bfad-43e2-aee5-d3c51ddb4b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt the draft model to generate speculated tokens\n",
        "\n",
        "`k=2`"
      ],
      "metadata": {
        "id": "WQ2MmnZgbuw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input prompt\n",
        "prompt = \"What is mitosis? Mitosis is the process by which a protein is broken\"\n",
        "\n",
        "k=6\n",
        "# we speculate next k tokens and store them along with their probs from draft model\n",
        "draft_next_tokens = []\n",
        "draft_next_token_probs = []\n",
        "\n",
        "for _ in range(k):\n",
        "  # Tokenize the prompt (turn it into token IDs)\n",
        "  encoded_prompt = tokenizer_draft(prompt, return_tensors='pt')\n",
        "\n",
        "  encoded_prompt = {key: value.to(device) for key, value in encoded_prompt.items()}\n",
        "\n",
        "  # Run the model and get the logits for the next token\n",
        "  with torch.no_grad():  # Disable gradient calculation during inference\n",
        "      outputs = model_draft(**encoded_prompt)\n",
        "      logits = outputs.logits\n",
        "\n",
        "  # Extract the logits for the next token (logits for the token after the input prompt)\n",
        "  next_token_logits = logits[0, -1, :]  # Logits for the next token (after the prompt)\n",
        "\n",
        "  # Apply softmax to convert logits into probabilities\n",
        "  probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "  # Get the top k most likely tokens and their probabilities\n",
        "  top_k = 10\n",
        "  top_token_probs, top_token_ids = torch.topk(probabilities, k=top_k)\n",
        "\n",
        "  # Decode the top k token IDs into human-readable tokens\n",
        "  top_token_strings = [tokenizer_draft.decode([token_id.item()]) for token_id in top_token_ids]\n",
        "\n",
        "  # Print the top 10 most likely next tokens and their probabilities\n",
        "  print(f\"Top {top_k} tokens and their likelihoods for the next token:\")\n",
        "  for i in range(top_k):\n",
        "      print(f\"Token: {top_token_strings[i]} | Probability: {top_token_probs[i].item():.4f}\")\n",
        "  print(\"\\n----------------------\\n\")\n",
        "  # Add the predicted token to the input prompt to predict the second positon\n",
        "  prompt = prompt + top_token_strings[0]\n",
        "\n",
        "  draft_next_tokens.append(top_token_strings[0])\n",
        "  draft_next_token_probs.append(top_token_probs[0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq_9TUMZb5XL",
        "outputId": "80f3ea85-2e74-404d-df55-6bfacab1c9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  down | Probability: 0.9311\n",
            "Token:  into | Probability: 0.0164\n",
            "Token:  up | Probability: 0.0151\n",
            "Token:  apart | Probability: 0.0037\n",
            "Token:  and | Probability: 0.0036\n",
            "Token:  in | Probability: 0.0032\n",
            "Token:  from | Probability: 0.0030\n",
            "Token: , | Probability: 0.0029\n",
            "Token:  out | Probability: 0.0028\n",
            "Token: . | Probability: 0.0026\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  into | Probability: 0.3629\n",
            "Token:  and | Probability: 0.1590\n",
            "Token:  by | Probability: 0.1014\n",
            "Token:  to | Probability: 0.0693\n",
            "Token:  in | Probability: 0.0563\n",
            "Token: , | Probability: 0.0507\n",
            "Token: . | Probability: 0.0501\n",
            "Token:  or | Probability: 0.0185\n",
            "Token:  from | Probability: 0.0141\n",
            "Token:  ( | Probability: 0.0083\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  its | Probability: 0.1434\n",
            "Token:  a | Probability: 0.0933\n",
            "Token:  smaller | Probability: 0.0542\n",
            "Token:  amino | Probability: 0.0452\n",
            "Token:  different | Probability: 0.0381\n",
            "Token:  two | Probability: 0.0351\n",
            "Token:  the | Probability: 0.0303\n",
            "Token:  small | Probability: 0.0298\n",
            "Token:  proteins | Probability: 0.0210\n",
            "Token:  molecules | Probability: 0.0183\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  constituent | Probability: 0.7329\n",
            "Token:  components | Probability: 0.0785\n",
            "Token:  parts | Probability: 0.0148\n",
            "Token:  component | Probability: 0.0129\n",
            "Token:  constituents | Probability: 0.0094\n",
            "Token:  amino | Probability: 0.0079\n",
            "Token:  derivatives | Probability: 0.0054\n",
            "Token:  own | Probability: 0.0046\n",
            "Token:  active | Probability: 0.0036\n",
            "Token:  functional | Probability: 0.0036\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  parts | Probability: 0.2695\n",
            "Token:  amino | Probability: 0.1507\n",
            "Token:  components | Probability: 0.1391\n",
            "Token:  proteins | Probability: 0.1030\n",
            "Token:  molecules | Probability: 0.0348\n",
            "Token:  elements | Probability: 0.0319\n",
            "Token:  bases | Probability: 0.0267\n",
            "Token:  substances | Probability: 0.0130\n",
            "Token:  compounds | Probability: 0.0095\n",
            "Token:  constituent | Probability: 0.0079\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token: . | Probability: 0.3097\n",
            "Token:  and | Probability: 0.1962\n",
            "Token: , | Probability: 0.1845\n",
            "Token:  by | Probability: 0.0497\n",
            "Token:  ( | Probability: 0.0350\n",
            "Token:  that | Probability: 0.0243\n",
            "Token:  to | Probability: 0.0186\n",
            "Token:  in | Probability: 0.0167\n",
            "Token:  or | Probability: 0.0127\n",
            "Token:  which | Probability: 0.0098\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate spculated tokens from Draft model & probs"
      ],
      "metadata": {
        "id": "mB2HjqrDdF1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Speculated tokens: {draft_next_tokens}\")\n",
        "print(f\"Speculated tokens probs: {draft_next_token_probs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg0kaznSdJw4",
        "outputId": "66e2ac4e-3f99-4d1d-89a0-4f96448163fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speculated tokens: [' down', ' into', ' its', ' constituent', ' parts', '.']\n",
            "Speculated tokens probs: [0.931113064289093, 0.36290252208709717, 0.14342136681079865, 0.7328725457191467, 0.26946306228637695, 0.3097057044506073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load target model"
      ],
      "metadata": {
        "id": "9jf_Dnr1bXbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GPT2-XL tokenizer and model\n",
        "tokenizer_target = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
        "model_target = GPT2LMHeadModel.from_pretrained('gpt2-xl')"
      ],
      "metadata": {
        "id": "l42j8eX-bY3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use target model for evaluation\n",
        "\n",
        "We pass all speculated tokens to target model get the tokens liklihood for accepting or rejecting them. We also generate one token as extra credit at the end!"
      ],
      "metadata": {
        "id": "rqo4UFt6db7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model and input tensors to the appropriate device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_target.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNemK6oMeHqB",
        "outputId": "aa3f4828-e896-465c-e835-7433d0f3cf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1600)\n",
              "    (wpe): Embedding(1024, 1600)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-47): 48 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
              "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
              "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model's vocabulary size (it should be 50257 for GPT2)\n",
        "vocab_size = model_target.config.vocab_size\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXBii59yeL7N",
        "outputId": "0f471bf0-db40-4fff-ff3f-abf323d0f545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add speculated token to prompt for evaluation!"
      ],
      "metadata": {
        "id": "LE96CKM9eN4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt with speculated tokens\n",
        "new_prompt = \"What is mitosis? Mitosis is the process by which a protein is broken down into its constituent parts.\"\n",
        "new_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdwrhIupdzIF",
        "outputId": "1874c136-1960-4abb-c464-0958154cb498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is mitosis? Mitosis is the process by which a protein is broken down into its constituent parts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_probs = []\n",
        "target_next_token = None\n",
        "\n",
        "# Tokenize the prompt\n",
        "encoded_prompt = tokenizer_target(new_prompt, return_tensors='pt')\n",
        "\n",
        "all_target_probs = []\n",
        "\n",
        "# Get model outputs (logits) for the input\n",
        "with torch.no_grad():  # Disable gradient computation to save memory\n",
        "    outputs = model_target(**encoded_prompt)\n",
        "    logits = outputs.logits  # Raw logits (before softmax)\n",
        "\n",
        "\n",
        "for inx in range(-1, -1-k-1, -1):\n",
        "  # Get the logits for the last token in the sequence (the next token prediction)\n",
        "  token_logits = logits[:, inx, :]  # The logits for the last token position\n",
        "\n",
        "  # Apply softmax to get probabilities of each token in the vocabulary\n",
        "  probabilities = torch.softmax(token_logits, dim=-1)\n",
        "\n",
        "  # Let's keep target probs from last token sampling as gift!\n",
        "  all_target_probs.append(probabilities)\n",
        "\n",
        "  # Get the token ID of the most likely next token\n",
        "  predicted_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "  # Get the probability (likelihood) of the predicted token\n",
        "  predicted_token_probability = probabilities[0, predicted_token_id].item()\n",
        "\n",
        "  # Decode the predicted token ID back to text\n",
        "  predicted_token = tokenizer_target.decode(predicted_token_id)\n",
        "\n",
        "  # Print the predicted next token and its likelihood (probability)\n",
        "  print(f\"Token '{inx}': '{predicted_token}' - Likelihood: {predicted_token_probability:.4f}\")\n",
        "  print(\"\\n----------------------\\n\")\n",
        "\n",
        "  # next target token\n",
        "  if inx == -1:\n",
        "    target_next_token = predicted_token\n",
        "  else:\n",
        "    target_probs.append(predicted_token_probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-IaOlg9v0jU",
        "outputId": "5c55b6b4-d0e5-4e9e-ac28-c55bae6a2319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token '-1': ' The' - Likelihood: 0.1212\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-2': '.' - Likelihood: 0.5073\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-3': ' parts' - Likelihood: 0.5429\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-4': ' constituent' - Likelihood: 0.2822\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-5': ' smaller' - Likelihood: 0.7594\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-6': ' into' - Likelihood: 0.5791\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-7': ' down' - Likelihood: 0.9197\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_probs.reverse()\n",
        "target_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEilHPVGv_sn",
        "outputId": "2ba0d043-e675-4c41-bf02-0d7a0e6e6f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9196940660476685,\n",
              " 0.5791090130805969,\n",
              " 0.7593812346458435,\n",
              " 0.2822127044200897,\n",
              " 0.5428709983825684,\n",
              " 0.5072702169418335]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_target_probs.reverse()"
      ],
      "metadata": {
        "id": "Ckq995tk9nQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Speculated tokens: {draft_next_tokens}\")\n",
        "print(f\"Speculated tokens probs: {draft_next_token_probs}\")"
      ],
      "metadata": {
        "id": "WrsvSGC77h26",
        "outputId": "fd1253c7-b440-48cf-e4f0-798158235573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speculated tokens: [' down', ' into', ' its', ' constituent', ' parts', '.']\n",
            "Speculated tokens probs: [0.931113064289093, 0.36290252208709717, 0.14342136681079865, 0.7328725457191467, 0.26946306228637695, 0.3097057044506073]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative sampling\n",
        "\n",
        "`p: draft likelihood for token x`\n",
        "\n",
        "`q: draft likelihood for token x`\n",
        "\n",
        "`Case 1: If q(x) >= p(x) then accept token x`\n",
        "\n",
        "`Case 2: If q(x) < p(x) then accept token x with probability of q(x)/p(x)`\n",
        "\n",
        "`As soon as we reject break from the loop and then sample from q()`\n"
      ],
      "metadata": {
        "id": "ONyfl24o5H9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "accepted_tokens = []\n",
        "for inx in range(k):\n",
        "  token = draft_next_tokens[inx]\n",
        "  p = draft_next_token_probs[inx]\n",
        "  q = target_probs[inx]\n",
        "\n",
        "  print(f\"inx: {inx}: p: {p} & q: {q}\")\n",
        "  print(f\"Evaluating: {token}\")\n",
        "\n",
        "  if q >= p:\n",
        "    print(f\"accepting!\\n\")\n",
        "    accepted_tokens.append(token)\n",
        "  else:\n",
        "    prob = q/p\n",
        "    print(f\"sampling with prob: {prob}\")\n",
        "    if random.random() <= prob:\n",
        "      print(f\"accepting!\\n\")\n",
        "      accepted_tokens.append(token)\n",
        "    else:\n",
        "      # break from the loop and sample next token from q\n",
        "      print(\"breaking from loop!!\")\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-QrDh25v0Ty",
        "outputId": "cc77b674-924e-49c5-a889-040f40fb4ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inx: 0: p: 0.931113064289093 & q: 0.9196940660476685\n",
            "Evaluating:  down\n",
            "sampling with prob: 0.9877361851322073\n",
            "accepting!\n",
            "\n",
            "inx: 1: p: 0.36290252208709717 & q: 0.5791090130805969\n",
            "Evaluating:  into\n",
            "accepting!\n",
            "\n",
            "inx: 2: p: 0.14342136681079865 & q: 0.7593812346458435\n",
            "Evaluating:  its\n",
            "accepting!\n",
            "\n",
            "inx: 3: p: 0.7328725457191467 & q: 0.2822127044200897\n",
            "Evaluating:  constituent\n",
            "sampling with prob: 0.38507746820173555\n",
            "breaking from loop!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inx"
      ],
      "metadata": {
        "id": "-Q6zPXYx-gMD",
        "outputId": "1b8eaeb5-98bf-4f63-bb08-82771b0043d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = all_target_probs[inx]\n",
        "\n",
        "# Get the token ID of the most likely next token\n",
        "predicted_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "# Get the probability (likelihood) of the predicted token\n",
        "predicted_token_probability = probabilities[0, predicted_token_id].item()\n",
        "\n",
        "# Decode the predicted token ID back to text\n",
        "predicted_token = tokenizer_target.decode(predicted_token_id)\n",
        "\n",
        "# Print the predicted next token and its likelihood (probability)\n",
        "print(f\"Token '{predicted_token}': '{predicted_token}' - Likelihood: {predicted_token_probability:.4f}\")"
      ],
      "metadata": {
        "id": "OgItJ4rS8lLG",
        "outputId": "a2bf04ea-4125-47c5-e241-7c1e4479ebad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ' constituent': ' constituent' - Likelihood: 0.2822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSLzRbnM9ujJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}