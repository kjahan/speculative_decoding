{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjahan/speculative_decoding/blob/main/notebooks/speculative_sampling_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative Sampling\n",
        "\n",
        "Here our goal is to speed up generative model inference time. This has many use cases for edit suggestion for writing or coding.\n",
        "\n",
        "We will use a draft/basic decoder like GPT-2 and a bigger size model as the core model. We will prompt the draft model and generate k speculative tokens along with their probabilities.\n",
        "\n",
        "Next we feed those k tokens along with the original prompt to the main model to get their liklihhods at once from the attention mask layer. Then we use the probabilities for speculative tokens from the draft model and main model to accept or reject speculated tokens.\n",
        "\n",
        "See this video for more explanations:\n",
        "\n",
        "https://www.youtube.com/watch?v=S-8yr_RibJ4\n",
        "\n",
        "The key insight is that there are many simple tokens like \"of\" that even smaller model can easily predict them so we can use the smaller model to generate them faster and then use the bigger size model for facts and harder tokens!\n",
        "\n",
        "https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/\n",
        "\n",
        "https://www.youtube.com/watch?v=9wNAgpX6z_4\n",
        "\n",
        "https://docs.google.com/presentation/d/1p1xE-EbSAnXpTSiSI0gmy_wdwxN5XaULO3AnCWWoRe4/edit#slide=id.p\n",
        "\n",
        "### VLLM speculative decoding\n",
        "\n",
        "https://docs.vllm.ai/en/stable/features/reasoning_outputs.html"
      ],
      "metadata": {
        "id": "CxOfeD1PkeYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step I: Load Draft LLM (opt-125m)"
      ],
      "metadata": {
        "id": "kJBAkfFAa4hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KfO29WW_GwXi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draft_model_name = \"facebook/opt-125m\"  # Small model\n",
        "\n",
        "# Load models and tokenizers\n",
        "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name)\n",
        "draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
        "\n",
        "# Move model and input to the same device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "draft_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDYLeeven4Zv",
        "outputId": "1737d906-7848-40a8-983f-75dca4398165"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x OPTDecoderLayer(\n",
              "          (self_attn): OPTSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKhXclgjHoSs",
        "outputId": "77bb73e8-827f-4dfa-efb1-27cc60627ca9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prompt the draft model to generate speculated tokens\n",
        "\n",
        "`k=5`"
      ],
      "metadata": {
        "id": "WQ2MmnZgbuw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_draf_model_2(prompt, k=5):\n",
        "  # we speculate next k tokens and store them along with their probs from draft model\n",
        "  draft_next_token_ids = []\n",
        "  draft_tokens_probs = []\n",
        "\n",
        "  for pos in range(k):\n",
        "    # Tokenize the prompt (turn it into token IDs)\n",
        "    encoded_prompt = draft_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    encoded_prompt = {key: value.to(device) for key, value in encoded_prompt.items()}\n",
        "\n",
        "    # Run the model and get the logits for the next token\n",
        "    with torch.no_grad():  # Disable gradient calculation during inference\n",
        "        outputs = draft_model(**encoded_prompt)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Extract the logits for the next token (logits for the token after the input prompt)\n",
        "    next_token_logits = logits[0, -1, :]  # Logits for the next token (after the prompt)\n",
        "\n",
        "    # Apply softmax to convert logits into probabilities\n",
        "    probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Get the top k most likely tokens and their probabilities\n",
        "    top_k = 5\n",
        "    top_token_probs, top_token_ids = torch.topk(probabilities, k=top_k)\n",
        "    top_token = draft_tokenizer.decode([top_token_ids[0].item()])\n",
        "    print(f\"Next draft token: {top_token} --> likelihoods: {top_token_probs[0]}\")\n",
        "    # Add the predicted token to the input prompt to predict the second positon\n",
        "    prompt = prompt + top_token\n",
        "\n",
        "    draft_next_token_ids.append(top_token_ids[0].item())\n",
        "    draft_tokens_probs.append(probabilities.cpu().numpy())\n",
        "\n",
        "  results = {'draft_token_ids': draft_next_token_ids, 'probs': draft_tokens_probs}\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "2FS4xEFW211p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate draft proposal"
      ],
      "metadata": {
        "id": "aKb6Or2dIK-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input prompt\n",
        "# prompt = \"What is mitosis? Mitosis is the process by which a protein is broken\"\n",
        "# prompt = \"Mitosis is the process by\"\n",
        "prompt = \"Paris is the capital of\"\n",
        "k=5\n",
        "\n",
        "results = run_draf_model_2(prompt, k)\n",
        "\n",
        "draft_next_token_ids, draft_tokens_probs = results['draft_token_ids'], results['probs']"
      ],
      "metadata": {
        "id": "KZF2e1tj4f1I",
        "outputId": "e0ebc228-8a17-4a8c-8716-4b369dceaab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next draft token:  the --> likelihoods: 0.38205596804618835\n",
            "Next draft token:  French --> likelihoods: 0.24400585889816284\n",
            "Next draft token:  Republic --> likelihoods: 0.34184378385543823\n",
            "Next draft token: . --> likelihoods: 0.38570883870124817\n",
            "Next draft token: \n",
            " --> likelihoods: 0.22947268187999725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(draft_next_token_ids)\n",
        "#draft_tokens_probs"
      ],
      "metadata": {
        "id": "tTafhh3Z4fuP",
        "outputId": "ad2be315-642f-47b5-ecbd-5eafb070aa19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1515, 3497, 4, 50118]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Target LLM (opt-350m)"
      ],
      "metadata": {
        "id": "9jf_Dnr1bXbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OPT tokenizer and model\n",
        "target_model_name = \"facebook/opt-350m\"  # Larger model\n",
        "\n",
        "target_model = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
        "target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "target_model.to(device)"
      ],
      "metadata": {
        "id": "l42j8eX-bY3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1e9599-d3ba-4744-9c20-0a3e2287afe9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Run Target model for evaluation\n",
        "\n",
        "We pass all speculated tokens to target model get the tokens liklihood for accepting or rejecting them. We also generate one token as extra credit at the end!"
      ],
      "metadata": {
        "id": "rqo4UFt6db7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_ = draft_model.config.vocab_size\n",
        "vocab_size_"
      ],
      "metadata": {
        "id": "uwjfFYtk98lY",
        "outputId": "901f0729-04dd-4c04-e456-fcd4077d17fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50272"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model's vocabulary size (it should be 50257 for GPT2)\n",
        "vocab_size = target_model.config.vocab_size\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXBii59yeL7N",
        "outputId": "08896eba-845c-4699-c325-ff90ff9bb13f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50272"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KVopaUrLc45Q",
        "outputId": "62a30524-2563-44a0-f0f7-08223e9b5305"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add speculated token to prompt for evaluation!"
      ],
      "metadata": {
        "id": "LE96CKM9eN4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_all_token_probabilities(prompt):\n",
        "    \"\"\"\n",
        "    Given a prompt, this function returns the probability distributions for each token position\n",
        "    in the prompt, considering all tokens in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "    - prompt (str): The input prompt for the target model.\n",
        "\n",
        "    Returns:\n",
        "    - List of dictionaries: Each dictionary contains token position and all vocabulary tokens\n",
        "    likelihoods.\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    inputs = target_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Get the model outputs (logits)\n",
        "    with torch.no_grad():\n",
        "        logits = target_model(**inputs).logits\n",
        "\n",
        "    # Extract the logits for the tokens in the prompt\n",
        "    logits = logits.squeeze(0)  # Remove the batch dimension\n",
        "\n",
        "    # Get the token ids for the input prompt\n",
        "    input_ids = inputs[\"input_ids\"].squeeze(0)  # (sequence_length,)\n",
        "\n",
        "    # Calculate the probabilities of the tokens using softmax\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Store the probabilities for each position\n",
        "    position_probabilities = []\n",
        "\n",
        "    for i in range(len(input_ids)):\n",
        "        position_probs = probabilities[i].cpu().numpy()  # Get the probability distribution for this token position\n",
        "        position_probabilities.append({\n",
        "            'position': i,\n",
        "            'probs': position_probs\n",
        "        })\n",
        "\n",
        "    return position_probabilities"
      ],
      "metadata": {
        "id": "NEvuvjscDukl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run evaluation"
      ],
      "metadata": {
        "id": "-vDSUtJjJr3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast Inference from Transformers via Speculative Decoding\n",
        "\n",
        "We are implmenting Algorithm 1 described in the following paper:\n",
        "\n",
        "https://arxiv.org/pdf/2211.17192\n",
        "\n",
        "`p: Target model likelihood for token x`\n",
        "\n",
        "`q: Draft likelihood for token x`\n",
        "\n",
        "`Case 1: If p(x) >= q(x) then accept token x`\n",
        "\n",
        "`Case 2: If p(x) < q(x) then accept token x by flipping a coin with  probability of p(x)/q(x)`\n",
        "\n",
        "`As soon as we reject break from the loop and then if we haven't sample all k speculated tokens then sample one more token from norm(max(0, p(x)-q(x))))`\n"
      ],
      "metadata": {
        "id": "ONyfl24o5H9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function"
      ],
      "metadata": {
        "id": "5ZN-VEZFzVeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def adjust_and_sample(p, q):\n",
        "    \"\"\"\n",
        "    Adjusts the probability distribution p using q and then samples from the resulting distribution.\n",
        "\n",
        "    Args:\n",
        "    - p (numpy array): Original probability distribution p(x).\n",
        "    - q (numpy array): Comparison probability distribution q(x).\n",
        "\n",
        "    Returns:\n",
        "    - sampled_token (int): The index of the sampled token from the adjusted distribution.\n",
        "    \"\"\"\n",
        "    # Step 1: Subtract q(x) from p(x)\n",
        "    adjusted_distribution = p - q\n",
        "\n",
        "    # Step 2: Apply max(0, p(x) - q(x)) to ensure non-negative values\n",
        "    adjusted_distribution = np.maximum(0, adjusted_distribution)\n",
        "\n",
        "    # Step 3: Normalize the adjusted distribution to ensure it sums to 1\n",
        "    adjusted_distribution /= np.sum(adjusted_distribution)\n",
        "\n",
        "    # Step 4: Sample from the adjusted distribution\n",
        "    sampled_token = np.random.choice(len(p), p=adjusted_distribution)\n",
        "\n",
        "    return sampled_token"
      ],
      "metadata": {
        "id": "V2v3MNxBzW90"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# p(x) and q(x) are example probability distributions\n",
        "p = np.array([0.1, 0.2, 0.3, 0.4])  # Original distribution\n",
        "q = np.array([0.5, 0.1, 0.2, 0.2])  # Comparison distribution\n",
        "\n",
        "sampled_token = adjust_and_sample(p, q)\n",
        "print(f\"Sampled token index: {sampled_token}\")"
      ],
      "metadata": {
        "id": "O7vac0CgzYf8",
        "outputId": "a5cc7c50-0125-49c0-ecba-e0f7e5f09790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled token index: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run evaluation"
      ],
      "metadata": {
        "id": "1vsblsl_06TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_draft_tokens(draft_next_token_ids):\n",
        "  draft_next_tokens = []\n",
        "  for token_id in draft_next_token_ids:\n",
        "    draft_next_tokens.append(draft_tokenizer.decode([token_id]))\n",
        "  return draft_next_tokens"
      ],
      "metadata": {
        "id": "8LqmAmm_6oVy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_speculative_decoding(prompt, draft_next_token_ids, draft_tokens_probs):\n",
        "\n",
        "  new_prompt = prompt + ''.join(get_draft_tokens(draft_next_token_ids))\n",
        "  print(new_prompt+\"\\n\")\n",
        "\n",
        "  position_probabilities = get_target_all_token_probabilities(new_prompt)\n",
        "\n",
        "  # Tokenize the prompt\n",
        "  inputs = target_tokenizer(new_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # Get the token ids for the input prompt\n",
        "  input_ids = inputs[\"input_ids\"].squeeze(0)  # (sequence_length,)\n",
        "\n",
        "  accepted_tokens = []\n",
        "  # for inx in range(len(input_ids) - k, len(input_ids) ):  # start from 1 to avoid the <BOS> token (if any)\n",
        "  for pos, token_id  in enumerate(draft_next_token_ids):  # start from 1 to avoid the <BOS> token (if any)\n",
        "      # Draft model likelihood\n",
        "      token = draft_tokenizer.decode([token_id])\n",
        "      q = round(draft_tokens_probs[pos][token_id].item(), 4)\n",
        "      # Target model likelihood\n",
        "      p_inx = len(input_ids) - k + pos - 1\n",
        "      #print(f\"p_inx: {p_inx}\")\n",
        "      p = round(position_probabilities[p_inx]['probs'][token_id].item(), 4)\n",
        "      token = target_tokenizer.decode([token_id])\n",
        "      print(f\"Evaluating: {token}\")\n",
        "      print(f\"{token_id}: {token} --> p: {p} & q: {q}\")\n",
        "      if p >= q:\n",
        "        print(f\"accepting ...\\n\")\n",
        "        accepted_tokens.append(token)\n",
        "      else:\n",
        "        prob = p/q\n",
        "        print(f\"sampling with prob: {prob}\")\n",
        "        if random.random() <= prob:\n",
        "          print(f\"accepting ...\\n\")\n",
        "          accepted_tokens.append(token)\n",
        "        else:\n",
        "          # break from the loop and sample next token from q\n",
        "          print(f\"\\nRejecting - pos: {pos}!\")\n",
        "          # check if we are breaking early then sample from max(0,p(x)-q(x))\n",
        "          if pos < k-1:\n",
        "            # sample from norm(max(0, p-q))\n",
        "            ps = position_probabilities[p_inx]['probs']\n",
        "            qs = draft_tokens_probs[pos]\n",
        "            #print(ps)\n",
        "            #print(qs)\n",
        "            sampled_token_id = adjust_and_sample(ps, qs)\n",
        "            token = draft_tokenizer.decode([sampled_token_id])\n",
        "            accepted_tokens.append(token)\n",
        "            print(f\"Last sampled token: {token}\")\n",
        "\n",
        "          break\n",
        "  return accepted_tokens"
      ],
      "metadata": {
        "id": "WGbT07fX5y6T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run speculative decoing technique"
      ],
      "metadata": {
        "id": "sqfk9IjhKTj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "19hQfBegEHJJ",
        "outputId": "62fe0431-2d63-4f04-ae48-b0c2ef3ac5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris is the capital of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accepted_tokens = run_speculative_decoding(prompt, draft_next_token_ids, draft_tokens_probs)\n",
        "\n",
        "print(f\"accepted tokens: {accepted_tokens}\")"
      ],
      "metadata": {
        "id": "-a7Nl-lH7vb3",
        "outputId": "ad792ec2-c20d-4f61-c8c0-92048cc5c957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is the capital of the French Republic.\n",
            "\n",
            "\n",
            "Evaluating:  the\n",
            "5:  the --> p: 0.2845 & q: 0.3821\n",
            "sampling with prob: 0.7445694844281601\n",
            "\n",
            "Rejecting - pos: 0!\n",
            "Last sampled token:  France\n",
            "accepted tokens: [' France']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSLzRbnM9ujJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}