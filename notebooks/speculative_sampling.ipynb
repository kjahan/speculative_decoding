{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjahan/speculative_decoding/blob/main/notebooks/speculative_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative Sampling\n",
        "\n",
        "Here our goal is to speed up generative model inference time. This has many use cases for edit suggestion for writing or coding.\n",
        "\n",
        "We will use a draft/basic decoder like GPT-2 and a bigger size model as the core model. We will prompt the draft model and generate k speculative tokens along with their probabilities.\n",
        "\n",
        "Next we feed those k tokens along with the original prompt to the main model to get their liklihhods at once from the attention mask layer. Then we use the probabilities for speculative tokens from the draft model and main model to accept or reject speculated tokens.\n",
        "\n",
        "See this video for more explanations:\n",
        "\n",
        "https://www.youtube.com/watch?v=S-8yr_RibJ4\n",
        "\n",
        "The key insight is that there are many simple tokens like \"of\" that even smaller model can easily predict them so we can use the smaller model to generate them faster and then use the bigger size model for facts and harder tokens!\n",
        "\n",
        "https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/\n",
        "\n",
        "https://www.youtube.com/watch?v=9wNAgpX6z_4\n",
        "\n",
        "https://docs.google.com/presentation/d/1p1xE-EbSAnXpTSiSI0gmy_wdwxN5XaULO3AnCWWoRe4/edit#slide=id.p\n",
        "\n",
        "### VLLM speculative decoding\n",
        "\n",
        "https://docs.vllm.ai/en/stable/features/reasoning_outputs.html"
      ],
      "metadata": {
        "id": "CxOfeD1PkeYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load draft model\n",
        "\n",
        "`GPT-2`"
      ],
      "metadata": {
        "id": "kJBAkfFAa4hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "04yqPITqmCfk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained tokenizer and model for text generation\n",
        "tokenizer_draft = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model_draft = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Move model and input to the same device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_draft.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "5701615c3960403ca93d4aa1ef29236c",
            "527a44502ead4ab18d4a278dd94e4011",
            "e9ce257170094183b6375e0dc11782a3",
            "2ab443a9608141ed913c6fbf4a72081e",
            "34c468522c0b4649b6e705125b47b8df",
            "372877bc6a084c02a753d8a58f3d7e33",
            "77f2ed41bbff47bda0fc6222431b3acc",
            "1e9f70386d4f4df6b18d835aedddb212",
            "efcff889580c41be8b6c78d28d29f404",
            "7f93366610ef4f34ab199f81274a52a2",
            "72e5c33e3b774527af2b594419787331",
            "8c127c0d43af4353bee9831d3fa2c990",
            "120f769f575f47ca9e2de5297b3f083a",
            "1679588632f94e328a3da5ff5f2c3e9f",
            "51e2315c38b54c7b8a2ce0beb070d373",
            "a3c986a7fce540f9b1f80f4c4bad1989",
            "d85d49ac427b4891b5771806ffc4a3da",
            "43e51bce11f440f8ae15cd3b50196985",
            "504ec0b4d37d43fb97f03c1fbfebadc0",
            "50fa3f869ce74810943028c8ecdc8d76",
            "8041a1825a7b4f93afbe9dfb1f2bfc8a",
            "b197950232b94b62a4f6fc09b1473fe0",
            "96723b9accf4476ab081bc855a4426e5",
            "2e5e2ca8ba15452ca46e973e7d02edc1",
            "b35c8e92eed54eeda7543ba3eaf3bd8b",
            "c5244949362b4a228f59c525bcc03e46",
            "8895141255074d06ad82082cfe9e0e0d",
            "8b062d81615c4b98a1c9b506dd90e6ca",
            "8b64e9e203094c7caebdec00096145bc",
            "b07d3d51803943579f50d1f5c6b38816",
            "e32c65bae5fd4efeb43f3030715bd986",
            "af5958aed33b497b8a3c5f7eb372ca35",
            "5063b2f4c8da4f17b425be4f4b22925b",
            "186e4cf8f5414a319f3f6e08e5a6a0db",
            "739608183e4d4e0a8072a0e964fbaa29",
            "987581c5215e4a1b9c881e5aaef5fe06",
            "dbade50c38d9414fa8924b8ae4f01106",
            "2c2c259fa65c42ca88f16c8ffea84d75",
            "8b74f34d5d7d42e089c8bf882ce5a73f",
            "a9f7f08eaf9d48f58513c7f7bbd20628",
            "cefd7c1e786b4e5da782aa2a491bf3e6",
            "c06f70c245904419a58ae0ee7490b3e4",
            "f0f9db92554a48c2a065eb5dbef9f0af",
            "cbdb077a5ab94019a22011117c917d21",
            "db99050624d04673ac7c2fa1f9110a49",
            "d3b7b7d6adf34444b09681ac1710acd0",
            "58340c4d1c89444698e75d5cb57cb672",
            "bb9ab1d7bea743cd9528fe66c5a6a373",
            "9b8e8dc27bff4ecb86d236cf6c82df52",
            "b1f1b80de39b45c5ab503da04740f1b4",
            "d8462d63ab124fb789583177825c0377",
            "1a265e5dc80143d09d2f88d93f1efdbb",
            "153f5942c41a4b2fb5eecec395730a86",
            "ee8a792142204308a22e89e3753f5874",
            "43a383ee14394dd2a733a3c20047c4f8",
            "58db285f3218453cb454054d0c8df1b0",
            "31edf5e1b3424fceaa60ff2d44de634a",
            "291f661c81db40e09e5022bf72acea2f",
            "b5d196ade5894341b3ec720badd14809",
            "e0e17bcda0f84fa898a24e807f3d7a8a",
            "f4da571640b04146bff3959c9decebbe",
            "f1627f1d4d0849faa73688a55dea576f",
            "7be747fb57db4f128b76eae3265bc4c6",
            "b10edd1ffeb24aa588864550d110344b",
            "1040c78ea1bd412da03d7bd09834fd2f",
            "6388924ece934b4789dcd094660a7288",
            "8a09b4dfc00346d080c184cd2b425f23",
            "52d159996ed040f8a030d5380b7a66ee",
            "4ed60ee7b5f449e8b7704c0aa85ec97d",
            "c7e0becd21954d0384aef39c50649cc0",
            "51b3fd90bc724d8dbbf179b366bdb462",
            "856159ba656f4494960a66452f28cbf8",
            "e9e4b88675c34dffb4643dae00881d02",
            "42d3a8b186c14c7796b163893068782a",
            "39fba4684ba644f592a042291a3fff02",
            "342665c6c4d6486696f42f301173bee5",
            "7cf5777a98234587a8cf62c7024c7d63"
          ]
        },
        "id": "DDYLeeven4Zv",
        "outputId": "2ff0cc64-8dc9-4d13-a070-791943148724"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5701615c3960403ca93d4aa1ef29236c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c127c0d43af4353bee9831d3fa2c990"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96723b9accf4476ab081bc855a4426e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "186e4cf8f5414a319f3f6e08e5a6a0db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db99050624d04673ac7c2fa1f9110a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58db285f3218453cb454054d0c8df1b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a09b4dfc00346d080c184cd2b425f23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt the draft model to generate speculated tokens\n",
        "\n",
        "`k=5`"
      ],
      "metadata": {
        "id": "WQ2MmnZgbuw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_draf_model(prompt,k=5):\n",
        "  # we speculate next k tokens and store them along with their probs from draft model\n",
        "  draft_next_tokens = []\n",
        "  draft_next_token_probs = []\n",
        "\n",
        "  for _ in range(k):\n",
        "    # Tokenize the prompt (turn it into token IDs)\n",
        "    encoded_prompt = tokenizer_draft(prompt, return_tensors='pt')\n",
        "\n",
        "    encoded_prompt = {key: value.to(device) for key, value in encoded_prompt.items()}\n",
        "\n",
        "    # Run the model and get the logits for the next token\n",
        "    with torch.no_grad():  # Disable gradient calculation during inference\n",
        "        outputs = model_draft(**encoded_prompt)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Extract the logits for the next token (logits for the token after the input prompt)\n",
        "    next_token_logits = logits[0, -1, :]  # Logits for the next token (after the prompt)\n",
        "\n",
        "    # Apply softmax to convert logits into probabilities\n",
        "    probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    # Get the top k most likely tokens and their probabilities\n",
        "    top_k = 10\n",
        "    top_token_probs, top_token_ids = torch.topk(probabilities, k=top_k)\n",
        "\n",
        "    # Decode the top k token IDs into human-readable tokens\n",
        "    top_token_strings = [tokenizer_draft.decode([token_id.item()]) for token_id in top_token_ids]\n",
        "\n",
        "    # Print the top 10 most likely next tokens and their probabilities\n",
        "    print(f\"Top {top_k} tokens and their likelihoods for the next token:\")\n",
        "    for i in range(top_k):\n",
        "        print(f\"Token: {top_token_strings[i]} | Probability: {top_token_probs[i].item():.4f}\")\n",
        "    print(\"\\n----------------------\\n\")\n",
        "    # Add the predicted token to the input prompt to predict the second positon\n",
        "    prompt = prompt + top_token_strings[0]\n",
        "\n",
        "    draft_next_tokens.append(top_token_strings[0])\n",
        "    draft_next_token_probs.append(top_token_probs[0].item())\n",
        "\n",
        "  results = {'tokens': draft_next_tokens, 'likelihoods': draft_next_token_probs}\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "eO2ajRMQa_je"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input prompt\n",
        "prompt = \"What is mitosis? Mitosis is the process by which a protein is broken\"\n",
        "k=5\n",
        "\n",
        "results = run_draf_model(prompt, k)\n",
        "\n",
        "draft_next_tokens, draft_next_token_probs = results['tokens'], results['likelihoods']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq_9TUMZb5XL",
        "outputId": "831c0a3f-3a92-49db-aca7-592a1b755afa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  down | Probability: 0.9311\n",
            "Token:  into | Probability: 0.0164\n",
            "Token:  up | Probability: 0.0151\n",
            "Token:  apart | Probability: 0.0037\n",
            "Token:  and | Probability: 0.0036\n",
            "Token:  in | Probability: 0.0032\n",
            "Token:  from | Probability: 0.0030\n",
            "Token: , | Probability: 0.0029\n",
            "Token:  out | Probability: 0.0028\n",
            "Token: . | Probability: 0.0026\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  into | Probability: 0.3629\n",
            "Token:  and | Probability: 0.1590\n",
            "Token:  by | Probability: 0.1014\n",
            "Token:  to | Probability: 0.0693\n",
            "Token:  in | Probability: 0.0563\n",
            "Token: , | Probability: 0.0507\n",
            "Token: . | Probability: 0.0501\n",
            "Token:  or | Probability: 0.0185\n",
            "Token:  from | Probability: 0.0141\n",
            "Token:  ( | Probability: 0.0083\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  its | Probability: 0.1434\n",
            "Token:  a | Probability: 0.0933\n",
            "Token:  smaller | Probability: 0.0542\n",
            "Token:  amino | Probability: 0.0452\n",
            "Token:  different | Probability: 0.0381\n",
            "Token:  two | Probability: 0.0351\n",
            "Token:  the | Probability: 0.0303\n",
            "Token:  small | Probability: 0.0298\n",
            "Token:  proteins | Probability: 0.0210\n",
            "Token:  molecules | Probability: 0.0183\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  constituent | Probability: 0.7329\n",
            "Token:  components | Probability: 0.0785\n",
            "Token:  parts | Probability: 0.0148\n",
            "Token:  component | Probability: 0.0129\n",
            "Token:  constituents | Probability: 0.0094\n",
            "Token:  amino | Probability: 0.0079\n",
            "Token:  derivatives | Probability: 0.0054\n",
            "Token:  own | Probability: 0.0046\n",
            "Token:  active | Probability: 0.0036\n",
            "Token:  functional | Probability: 0.0036\n",
            "\n",
            "----------------------\n",
            "\n",
            "Top 10 tokens and their likelihoods for the next token:\n",
            "Token:  parts | Probability: 0.2695\n",
            "Token:  amino | Probability: 0.1507\n",
            "Token:  components | Probability: 0.1391\n",
            "Token:  proteins | Probability: 0.1030\n",
            "Token:  molecules | Probability: 0.0348\n",
            "Token:  elements | Probability: 0.0319\n",
            "Token:  bases | Probability: 0.0267\n",
            "Token:  substances | Probability: 0.0130\n",
            "Token:  compounds | Probability: 0.0095\n",
            "Token:  constituent | Probability: 0.0079\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate spculated tokens from Draft model & probs"
      ],
      "metadata": {
        "id": "mB2HjqrDdF1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Speculated tokens: {draft_next_tokens}\")\n",
        "print(f\"Speculated tokens probs: {draft_next_token_probs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg0kaznSdJw4",
        "outputId": "d34ceff2-375d-400e-edb5-25159e11f9af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speculated tokens: [' down', ' into', ' its', ' constituent', ' parts']\n",
            "Speculated tokens probs: [0.9311023950576782, 0.36289504170417786, 0.14342203736305237, 0.7328733801841736, 0.26945626735687256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load target model"
      ],
      "metadata": {
        "id": "9jf_Dnr1bXbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GPT2-XL tokenizer and model\n",
        "tokenizer_target = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
        "model_target = GPT2LMHeadModel.from_pretrained('gpt2-xl')"
      ],
      "metadata": {
        "id": "l42j8eX-bY3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "2ffc523cc29648aa8ca9f871803a4b21",
            "8d48c687285a4260ba0266126fd17deb",
            "9a42c3081dc64181979de63f8a907911",
            "53ac018b70c94f29820d25c405c8c5e0",
            "8d2f8c53143d43a5b473e0b2613b9c6a",
            "9927ebfe088d459daa6e28b175e0c650",
            "9a84290080b24e609cff562cb35a88e3",
            "c8b363a6a431450a98dc39955fc056fa",
            "183ae5741c924eb58fba9c338b817624",
            "ab1c250408e34183a1edb11238146060",
            "8b033c0f34d644c5acc2026f6a09c694",
            "c934fcedd3a84293a072bd781d9bfd55",
            "096d64a464e64692aea0e9e4267dbdd0",
            "f9d075fb76fe4190aa3f4ca51afa3b56",
            "39b555464fc249218200c94297aa94c3",
            "515bd05d918640dc9dbfab0f5dc416f6",
            "e2b3aee077644ee4b928f6407106e8eb",
            "b3aa58324a964c6c8abe5fc3433d4cff",
            "a1d18a92e4c44624852dd472af02528d",
            "ea9af89a962f4533a7d857852d7d8b71",
            "c1d028ad2922404bb886d0e829c782d7",
            "d99cbc292f6d426fac19bcaa8f81c595",
            "b8866fceb87b485e9b4246f97eed3f7c",
            "d07084ffd75d4e3aae34eb475cc77953",
            "f22396b2779945a498f19add6822b8a9",
            "c208983ed616453cbd72f073a417ea7b",
            "e32718d697394044a67023df28a68478",
            "22ba4ad61a9242c29a1dec56618e0d94",
            "22fa41bf7acd44c7a0e4b52b6351b354",
            "9601fda9b928470dbac346330e2653a1",
            "e93b41a20f614784940c8ca97560f636",
            "5666547f7f11451f8ef240239064bad0",
            "4914165a24a04a73a1aba3d9c70c3ec3",
            "fccdeb6efca44946af6418033529f489",
            "5759062922034e95b88d2c72e3833b79",
            "93298bb7d40a457ba4f8e017b0798315",
            "b6dada26a25a4563871d3c068e02b348",
            "057f329788db46aca5faa77bf44842f6",
            "bbee00dafba24f3d8e80274fbd28b7d4",
            "474160dd2006444b9507330c0fe48e67",
            "6ba045dafb854b57b450a26a0d1df239",
            "ff61c7ee444145e1899bd80bcd0d7cb4",
            "f54ae8a06c9446eda8cc36a6577b0f3b",
            "04ac7a50753249708b1924b307e0b970",
            "fa0b5ce9778b4c01ba08b2057c47de12",
            "ed0317ce9edd47abba903f92f4c2e66c",
            "2bb3abbca21b48f297787847a5dadae3",
            "b198ab7e60e24b8ab50139974b5eaaa6",
            "80fe413d0e4b44caa633ccb23de25f12",
            "396d8416388d4dcdb90ab4674e5e1b10",
            "81f188e463464068b02730c70a97a5b8",
            "f21f982a24ee49eaa39b638181aa9dde",
            "a7c13516fabe4ae2a4f82d5017c814fa",
            "da48c7392d104d66b4e6c293b73ba764",
            "ff52d06ef67648ea82e58982e99075f3",
            "c4f94bb43f16413e87f6d688c5f4c347",
            "925c5016c3c845dbb8b5a9a2997f9afc",
            "7b8328cc488e400a8fd04d23ce8a0e74",
            "a468f6f8c0924979854cfa2597092426",
            "4ca224cbbb264f8ea1537c15666d7a0d",
            "e2d8c8348b4748d08148002828a06916",
            "9cdbe02ba01e4d47836cf53ed7ba2ad6",
            "342968fdbdb944778c3eefd973aa8f58",
            "8aac891e34374149bf03e4bde94af023",
            "1c35244d0fb54524978c2ec6260ba581",
            "2404bee1238c4aab9d769a6fab504e93",
            "7db465d001464e6090338703d8a8c5ff",
            "7f5328859be743708acaaf16fa279697",
            "ad122aa0939041e4be3f4de9e2b8f00c",
            "9121050d49df4f20bc3e93f93fcb0b53",
            "5693dd1ab19442478d3f2d4e3300d959",
            "ae5d083041074f2f9df57bbf29fc61f5",
            "47bcaa99513b423a91ac61d778705337",
            "3bf3d5f9b91d4e06b0d8873193c1e705",
            "eebbc683acbd4627936d66a1d9df5cf1",
            "47764f12917d46c39574f191b0b14e1c",
            "7ef0c98f8a7e4906adf6c275474ce9b9"
          ]
        },
        "outputId": "2b3403da-fab2-48f9-e444-36d34aad4593"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ffc523cc29648aa8ca9f871803a4b21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c934fcedd3a84293a072bd781d9bfd55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8866fceb87b485e9b4246f97eed3f7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fccdeb6efca44946af6418033529f489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa0b5ce9778b4c01ba08b2057c47de12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4f94bb43f16413e87f6d688c5f4c347"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7db465d001464e6090338703d8a8c5ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use target model for evaluation\n",
        "\n",
        "We pass all speculated tokens to target model get the tokens liklihood for accepting or rejecting them. We also generate one token as extra credit at the end!"
      ],
      "metadata": {
        "id": "rqo4UFt6db7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model and input tensors to the appropriate device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_target.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNemK6oMeHqB",
        "outputId": "cf927bc8-fe65-49f0-ed1d-f385b845edae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1600)\n",
              "    (wpe): Embedding(1024, 1600)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-47): 48 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
              "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
              "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model's vocabulary size (it should be 50257 for GPT2)\n",
        "vocab_size = model_target.config.vocab_size\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXBii59yeL7N",
        "outputId": "81a58de7-7ca7-494e-d6ec-9d05f7626998"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVopaUrLc45Q",
        "outputId": "cc28a5d7-bebc-4685-df2b-b7b64fb94b77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add speculated token to prompt for evaluation!"
      ],
      "metadata": {
        "id": "LE96CKM9eN4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_target_model(new_prompt):\n",
        "  target_probs = []\n",
        "  target_next_token = None\n",
        "\n",
        "  # # Tokenize the prompt\n",
        "  # encoded_prompt = tokenizer_target(new_prompt, return_tensors='pt')\n",
        "  # Tokenize the input and move the tensors to the same device as the model\n",
        "  encoded_prompt = tokenizer_target(new_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  all_target_probs = []\n",
        "\n",
        "  # Get model outputs (logits) for the input\n",
        "  with torch.no_grad():  # Disable gradient computation to save memory\n",
        "      outputs = model_target(**encoded_prompt)\n",
        "      logits = outputs.logits  # Raw logits (before softmax)\n",
        "\n",
        "\n",
        "  for inx in range(-1, -1-k-1, -1):\n",
        "    # Get the logits for the last token in the sequence (the next token prediction)\n",
        "    token_logits = logits[:, inx, :]  # The logits for the last token position\n",
        "\n",
        "    # Apply softmax to get probabilities of each token in the vocabulary\n",
        "    probabilities = torch.softmax(token_logits, dim=-1)\n",
        "\n",
        "    # Let's keep target probs from last token sampling as gift!\n",
        "    all_target_probs.append(probabilities)\n",
        "\n",
        "    # Get the token ID of the most likely next token\n",
        "    predicted_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    # Get the probability (likelihood) of the predicted token\n",
        "    predicted_token_probability = probabilities[0, predicted_token_id].item()\n",
        "\n",
        "    # Decode the predicted token ID back to text\n",
        "    predicted_token = tokenizer_target.decode(predicted_token_id)\n",
        "\n",
        "    # Print the predicted next token and its likelihood (probability)\n",
        "    print(f\"Token '{inx}': '{predicted_token}' - Likelihood: {predicted_token_probability:.4f}\")\n",
        "    print(\"\\n----------------------\\n\")\n",
        "\n",
        "    # next target token\n",
        "    if inx == -1:\n",
        "      target_next_token = predicted_token\n",
        "    else:\n",
        "      target_probs.append(predicted_token_probability)\n",
        "\n",
        "  results = {'likelihoods': target_probs, 'all_likelihoods': all_target_probs}\n",
        "  return results"
      ],
      "metadata": {
        "id": "um9JNikPb25U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt with speculated tokens\n",
        "new_prompt = \"What is mitosis? Mitosis is the process by which a protein is broken down into its constituent parts.\"\n",
        "new_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdwrhIupdzIF",
        "outputId": "30c36d7e-40ab-4856-be3c-c22a3ca2e926"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is mitosis? Mitosis is the process by which a protein is broken down into its constituent parts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_2 = run_target_model(new_prompt)\n",
        "\n",
        "target_probs, all_target_probs = results_2['likelihoods'], results_2['all_likelihoods']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcW8Rfxckx2",
        "outputId": "7f53f38e-63ab-4f09-b5ad-e2b394faac02"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token '-1': ' The' - Likelihood: 0.1212\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-2': '.' - Likelihood: 0.5073\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-3': ' parts' - Likelihood: 0.5429\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-4': ' constituent' - Likelihood: 0.2822\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-5': ' smaller' - Likelihood: 0.7594\n",
            "\n",
            "----------------------\n",
            "\n",
            "Token '-6': ' into' - Likelihood: 0.5791\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_probs.reverse()\n",
        "target_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEilHPVGv_sn",
        "outputId": "c00e81ea-8ee5-43e3-cae3-b09241b27964"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5790964365005493,\n",
              " 0.7593656182289124,\n",
              " 0.282208114862442,\n",
              " 0.5428615808486938,\n",
              " 0.5072563886642456]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_target_probs.reverse()"
      ],
      "metadata": {
        "id": "Ckq995tk9nQ7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Speculated tokens: {draft_next_tokens}\")\n",
        "print(f\"Speculated tokens probs: {draft_next_token_probs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ybhcOFsdvGm",
        "outputId": "96c5990c-fbe1-4251-cfe9-76e8dfa79c58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speculated tokens: [' down', ' into', ' its', ' constituent', ' parts']\n",
            "Speculated tokens probs: [0.9311023950576782, 0.36289504170417786, 0.14342203736305237, 0.7328733801841736, 0.26945626735687256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative sampling\n",
        "\n",
        "`p: Draft/Small model likelihood for token x`\n",
        "\n",
        "`q: Target/Large model likelihood for token x`\n",
        "\n",
        "`Case 1: If q(x) >= p(x) then accept token x`\n",
        "\n",
        "`Case 2: If q(x) < p(x) then accept token x by flipping a coin with  probability of q(x)/p(x)`\n",
        "\n",
        "`As soon as we reject break from the loop and then sample from q()`\n"
      ],
      "metadata": {
        "id": "ONyfl24o5H9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_speculative_decoding(draft_next_tokens, draft_next_token_probs, target_probs):\n",
        "  accepted_tokens = []\n",
        "  for inx in range(k):\n",
        "    token = draft_next_tokens[inx]\n",
        "    p = draft_next_token_probs[inx]\n",
        "    q = target_probs[inx]\n",
        "\n",
        "    print(f\"inx: {inx}: p: {p} & q: {q}\")\n",
        "    print(f\"Evaluating: {token}\")\n",
        "\n",
        "    if q >= p:\n",
        "      print(f\"accepting!\\n\")\n",
        "      accepted_tokens.append(token)\n",
        "    else:\n",
        "      prob = q/p\n",
        "      print(f\"sampling with prob: {prob}\")\n",
        "      if random.random() <= prob:\n",
        "        print(f\"accepting!\\n\")\n",
        "        accepted_tokens.append(token)\n",
        "      else:\n",
        "        # break from the loop and sample next token from q\n",
        "        print(\"breaking from loop!!\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bh6ikJsWd6j9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_speculative_decoding(draft_next_tokens, draft_next_token_probs, target_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GML2l_RxeLFL",
        "outputId": "159a9ea5-c333-4428-de31-07d8320d0bb0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inx: 0: p: 0.9311023950576782 & q: 0.5790964365005493\n",
            "Evaluating:  down\n",
            "sampling with prob: 0.6219471022461246\n",
            "accepting!\n",
            "\n",
            "inx: 1: p: 0.36289504170417786 & q: 0.7593656182289124\n",
            "Evaluating:  into\n",
            "accepting!\n",
            "\n",
            "inx: 2: p: 0.14342203736305237 & q: 0.282208114862442\n",
            "Evaluating:  its\n",
            "accepting!\n",
            "\n",
            "inx: 3: p: 0.7328733801841736 & q: 0.5428615808486938\n",
            "Evaluating:  constituent\n",
            "sampling with prob: 0.740730384711573\n",
            "accepting!\n",
            "\n",
            "inx: 4: p: 0.26945626735687256 & q: 0.5072563886642456\n",
            "Evaluating:  parts\n",
            "accepting!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inx=3\n",
        "probabilities = all_target_probs[inx]\n",
        "\n",
        "# Get the token ID of the most likely next token\n",
        "predicted_token_id = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "# Get the probability (likelihood) of the predicted token\n",
        "predicted_token_probability = probabilities[0, predicted_token_id].item()\n",
        "\n",
        "# Decode the predicted token ID back to text\n",
        "predicted_token = tokenizer_target.decode(predicted_token_id)\n",
        "\n",
        "# Print the predicted next token and its likelihood (probability)\n",
        "print(f\"Token '{predicted_token}': '{predicted_token}' - Likelihood: {predicted_token_probability:.4f}\")"
      ],
      "metadata": {
        "id": "OgItJ4rS8lLG",
        "outputId": "ae72993d-1ed6-4046-bc56-ba15b771645b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ' parts': ' parts' - Likelihood: 0.5429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSLzRbnM9ujJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}